# ã‚³ãƒ2å®Ÿè£…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: BERTå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Cursor Composerã«ã‚³ãƒ”ãƒ¼ã—ã¦å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

---

## ğŸ¤– å®Ÿè£…ä¾é ¼

`02_train_bert.py` ã‚’ä½œæˆã—ã€BERTã«ã‚ˆã‚‹äºŒå€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

---

## ğŸ“‹ å®Ÿè£…ä»•æ§˜

### ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: `cl-tohoku/bert-base-japanese-v3`ï¼ˆæ±åŒ—å¤§å­¦BERTï¼‰
- **ã‚¿ã‚¹ã‚¯**: äºŒå€¤åˆ†é¡ï¼ˆLabel 0: æ˜ç¢º / Label 1: æ›–æ˜§ï¼‰

### å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆæœŸè¨­å®šï¼‰

```python
TRAINING_CONFIG = {
    'model_name': 'cl-tohoku/bert-base-japanese-v3',
    'max_length': 128,  # ã‚³ãƒ1ã®eda_report.mdã‹ã‚‰èª¿æ•´
    'batch_size': 16,   # GPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´
    'learning_rate': 2e-5,
    'num_epochs': 3,
    'weight_decay': 0.01,
    'warmup_steps': 100,
    'output_dir': './results',
    'logging_steps': 50,
    'eval_steps': 100,
    'save_steps': 100,
    'seed': 42
}
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- `train.jsonl` (1,600ã‚µãƒ³ãƒ—ãƒ«)
- `val.jsonl` (200ã‚µãƒ³ãƒ—ãƒ«)
- `test.jsonl` (200ã‚µãƒ³ãƒ—ãƒ«)

---

## ğŸ”§ ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ§‹é€ 

ä»¥ä¸‹ã®æ§‹é€ ã§ `02_train_bert.py` ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

```python
"""
BERTäºŒå€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤–å›½äººåŠ´åƒè€…å‘ã‘ãƒ“ã‚¸ãƒã‚¹æŒ‡ç¤ºæ–‡ æ›–æ˜§æ€§åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
from datetime import datetime
import numpy as np
import torch
from datasets import Dataset, DatasetDict
from transformers import (
    BertJapaneseTokenizer,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ========================================
# è¨­å®š
# ========================================

CONFIG = {
    'model_name': 'cl-tohoku/bert-base-japanese-v3',
    'max_length': 128,
    'batch_size': 16,
    'learning_rate': 2e-5,
    'num_epochs': 3,
    'weight_decay': 0.01,
    'warmup_steps': 100,
    'output_dir': './results',
    'logging_dir': './logs',
    'seed': 42
}

# ========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ========================================

def load_jsonl(filepath):
    """
    JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    """
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def create_dataset():
    """
    HuggingFace Datasetå½¢å¼ã«å¤‰æ›
    """
    # TODO: å®Ÿè£…ã—ã¦ãã ã•ã„
    # train.jsonl, val.jsonl, test.jsonl ã‚’èª­ã¿è¾¼ã¿
    # DatasetDictå½¢å¼ã«å¤‰æ›
    pass

# ========================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
# ========================================

def tokenize_function(examples, tokenizer, max_length):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    """
    # TODO: å®Ÿè£…ã—ã¦ãã ã•ã„
    pass

# ========================================
# è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# ========================================

def compute_metrics(pred):
    """
    Accuracy, Precision, Recall, F1ã‚’è¨ˆç®—
    """
    # TODO: å®Ÿè£…ã—ã¦ãã ã•ã„
    # pred.label_ids ã¨ pred.predictions ã‹ã‚‰è¨ˆç®—
    pass

# ========================================
# æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–
# ========================================

def plot_confusion_matrix(y_true, y_pred, save_path):
    """
    æ··åŒè¡Œåˆ—ã‚’ä½œæˆãƒ»ä¿å­˜
    """
    # TODO: å®Ÿè£…ã—ã¦ãã ã•ã„
    pass

# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

def main():
    print("="*60)
    print("BERTå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    print("="*60)
    
    # ã‚·ãƒ¼ãƒ‰å›ºå®š
    torch.manual_seed(CONFIG['seed'])
    np.random.seed(CONFIG['seed'])
    
    # GPUç¢ºèª
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("\n[1/6] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
    dataset = create_dataset()
    print(f"Train samples: {len(dataset['train'])}")
    print(f"Val samples: {len(dataset['validation'])}")
    print(f"Test samples: {len(dataset['test'])}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    print("\n[2/6] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–ä¸­...")
    tokenizer = BertJapaneseTokenizer.from_pretrained(CONFIG['model_name'])
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    print("\n[3/6] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
    tokenized_datasets = dataset.map(
        lambda x: tokenize_function(x, tokenizer, CONFIG['max_length']),
        batched=True
    )
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\n[4/6] ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = BertForSequenceClassification.from_pretrained(
        CONFIG['model_name'],
        num_labels=2
    )
    model.to(device)
    
    # å­¦ç¿’è¨­å®š
    print("\n[5/6] å­¦ç¿’è¨­å®šä¸­...")
    training_args = TrainingArguments(
        output_dir=CONFIG['output_dir'],
        evaluation_strategy='epoch',
        save_strategy='epoch',
        learning_rate=CONFIG['learning_rate'],
        per_device_train_batch_size=CONFIG['batch_size'],
        per_device_eval_batch_size=CONFIG['batch_size'],
        num_train_epochs=CONFIG['num_epochs'],
        weight_decay=CONFIG['weight_decay'],
        warmup_steps=CONFIG['warmup_steps'],
        logging_dir=CONFIG['logging_dir'],
        logging_steps=50,
        load_best_model_at_end=True,
        metric_for_best_model='f1',
        seed=CONFIG['seed'],
        report_to='none'  # TensorBoardãªã©ã‚’ä½¿ã‚ãªã„å ´åˆ
    )
    
    # TraineråˆæœŸåŒ–
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets['train'],
        eval_dataset=tokenized_datasets['validation'],
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )
    
    # å­¦ç¿’é–‹å§‹
    print("\n[6/6] å­¦ç¿’é–‹å§‹...")
    print("-"*60)
    train_result = trainer.train()
    
    # å­¦ç¿’çµæœã®è¡¨ç¤º
    print("\n" + "="*60)
    print("å­¦ç¿’å®Œäº†ï¼")
    print("="*60)
    print(f"å­¦ç¿’æ™‚é–“: {train_result.metrics['train_runtime']:.2f}ç§’")
    print(f"æœ€çµ‚Loss: {train_result.metrics['train_loss']:.4f}")
    
    # Validationãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    print("\n--- Validationçµæœ ---")
    val_results = trainer.evaluate()
    for key, value in val_results.items():
        print(f"{key}: {value:.4f}")
    
    # Testãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    print("\n--- Testçµæœ ---")
    test_results = trainer.evaluate(tokenized_datasets['test'])
    for key, value in test_results.items():
        print(f"{key}: {value:.4f}")
    
    # æ··åŒè¡Œåˆ—ã®ä½œæˆ
    print("\næ··åŒè¡Œåˆ—ã‚’ä½œæˆä¸­...")
    predictions = trainer.predict(tokenized_datasets['test'])
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_true = predictions.label_ids
    
    plot_confusion_matrix(y_true, y_pred, 'figures/confusion_matrix.png')
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("\nãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    model.save_pretrained('./saved_model')
    tokenizer.save_pretrained('./saved_model')
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    results_summary = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'config': CONFIG,
        'train_loss': train_result.metrics['train_loss'],
        'val_results': val_results,
        'test_results': test_results
    }
    
    with open('training_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    print("\nâœ… ã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: ./saved_model")
    print(f"çµæœä¿å­˜å…ˆ: training_results.json")

if __name__ == '__main__':
    main()
```

---

## âœ… å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

### 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆcreate_dataseté–¢æ•°ï¼‰

```python
def create_dataset():
    train_data = load_jsonl('train.jsonl')
    val_data = load_jsonl('val.jsonl')
    test_data = load_jsonl('test.jsonl')
    
    # HuggingFace Datasetå½¢å¼ã«å¤‰æ›
    train_dataset = Dataset.from_dict({
        'text': [item['text'] for item in train_data],
        'label': [item['label'] for item in train_data]
    })
    
    val_dataset = Dataset.from_dict({
        'text': [item['text'] for item in val_data],
        'label': [item['label'] for item in val_data]
    })
    
    test_dataset = Dataset.from_dict({
        'text': [item['text'] for item in test_data],
        'label': [item['label'] for item in test_data]
    })
    
    dataset = DatasetDict({
        'train': train_dataset,
        'validation': val_dataset,
        'test': test_dataset
    })
    
    return dataset
```

### 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé–¢æ•°

```python
def tokenize_function(examples, tokenizer, max_length):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors=None  # Datasetã§ä½¿ã†ã¨ãã¯None
    )
```

### 3. è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary'
    )
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
```

### 4. æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–

```python
def plot_confusion_matrix(y_true, y_pred, save_path):
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ˜ç¢º(0)', 'æ›–æ˜§(1)'],
                yticklabels=['æ˜ç¢º(0)', 'æ›–æ˜§(1)'])
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    print(f"æ··åŒè¡Œåˆ—ã‚’ä¿å­˜: {save_path}")
```

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå¾Œã€ã¾ãšå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ï¼š

### ãƒ†ã‚¹ãƒˆç”¨å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

```python
# test_small.py
import json

# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰10ä»¶ãšã¤æŠ½å‡º
def create_small_dataset(input_file, output_file, n=10):
    with open(input_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data[:n]:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

create_small_dataset('train.jsonl', 'train_small.jsonl', 20)
create_small_dataset('val.jsonl', 'val_small.jsonl', 10)
create_small_dataset('test.jsonl', 'test_small.jsonl', 10)
```

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆ1 epochã€5åˆ†ç¨‹åº¦ï¼‰
python test_small.py
python 02_train_bert.py  # train.jsonlç­‰ã‚’train_small.jsonlç­‰ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ
```

---

## ğŸ“– training_guide.md ã®ä½œæˆ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ä¸€ç·’ã« `training_guide.md` ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

```markdown
# BERTå­¦ç¿’å®Ÿè¡Œã‚¬ã‚¤ãƒ‰

## å®Ÿè¡Œæ–¹æ³•

### é€šå¸¸å®Ÿè¡Œï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
```bash
python 02_train_bert.py
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã® `CONFIG` è¾æ›¸ã‚’ç·¨é›†ã—ã¦ãã ã•ã„ï¼š

- `max_length`: ãƒˆãƒ¼ã‚¯ãƒ³æœ€å¤§é•·ï¼ˆæ¨å¥¨: 128ï¼‰
- `batch_size`: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆGPU: 16, CPU: 8ï¼‰
- `learning_rate`: å­¦ç¿’ç‡ï¼ˆæ¨å¥¨: 2e-5ï¼‰
- `num_epochs`: ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆæ¨å¥¨: 3-5ï¼‰

## å­¦ç¿’æ™‚é–“ã®ç›®å®‰

- **GPUä½¿ç”¨æ™‚**: ç´„10-15åˆ†ï¼ˆ3 epochsï¼‰
- **CPUä½¿ç”¨æ™‚**: ç´„1-2æ™‚é–“ï¼ˆ3 epochsï¼‰

## æœŸå¾…ã•ã‚Œã‚‹ç²¾åº¦

- **åˆå›å­¦ç¿’**: Accuracy 70-80%
- **èª¿æ•´å¾Œ**: Accuracy 80-85%

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªä¸è¶³
â†’ batch_sizeã‚’8ã¾ãŸã¯4ã«æ¸›ã‚‰ã™

### å­¦ç¿’ãŒé€²ã¾ãªã„
â†’ learning_rateã‚’1e-5ã¾ãŸã¯5e-5ã«å¤‰æ›´

### éå­¦ç¿’
â†’ weight_decayã‚’0.1ã«å¢—ã‚„ã™ã€Early Stoppingã‚’æ´»ç”¨
```

---

## âœ… å®Œäº†ç¢ºèª

- [ ] `02_train_bert.py` ãŒä½œæˆã•ã‚ŒãŸ
- [ ] ã™ã¹ã¦ã®é–¢æ•°ãŒå®Ÿè£…ã•ã‚ŒãŸ
- [ ] å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒæˆåŠŸã—ãŸ
- [ ] `training_guide.md` ãŒä½œæˆã•ã‚ŒãŸ
- [ ] æ¬¡ã®ã‚³ãƒï¼ˆæœ¬æ ¼å­¦ç¿’ï¼‰ã®æº–å‚™ãŒæ•´ã£ãŸ

---

## â­ï¸ æ¬¡ã®ã‚³ãƒï¼ˆã‚³ãƒ3ï¼‰ã¸ã®æº–å‚™

å®Œäº†ã—ãŸã‚‰ã€æ¬¡ã®ã‚³ãƒ3ã§æœ¬æ ¼å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

æº–å‚™äº‹é …ï¼š
- ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
- å­¦ç¿’æ™‚é–“ã®è¦‹ç©ã‚‚ã‚Šã‚’æŠŠæ¡
- GPU/CPUã©ã¡ã‚‰ã§å®Ÿè¡Œã™ã‚‹ã‹æ±ºå®š










