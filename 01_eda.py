"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from transformers import BertJapaneseTokenizer
import os

# Seabornã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.family'] = 'DejaVu Sans'  # è‹±èªãƒ•ã‚©ãƒ³ãƒˆ

# figuresãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
os.makedirs('figures', exist_ok=True)

print("="*60)
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰")
print("="*60)

# ========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# ========================================

def load_jsonl(filepath):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

# ========================================
# ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®å¯è¦–åŒ–
# ========================================

def plot_label_distribution(data_dict):
    """ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’å¯è¦–åŒ–"""
    print("\n[1/5] ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®åˆ†æ...")
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for idx, (name, data) in enumerate(data_dict.items()):
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        
        ax = axes[idx]
        colors = ['#4CAF50', '#FF9800']
        bars = ax.bar(['Clear (Label 0)', 'Ambiguous (Label 1)'], 
                      [label_counts[0], label_counts[1]],
                      color=colors, alpha=0.8)
        
        ax.set_ylabel('Number of Samples', fontsize=12)
        ax.set_title(f'{name.capitalize()} Dataset\n(Total: {len(data)} samples)', 
                    fontsize=14, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}',
                   ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('figures/label_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/label_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    for name, data in data_dict.items():
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        print(f"    {name:12s}: ç·æ•°={len(data):4d}, Label 0={label_counts[0]:4d}, Label 1={label_counts[1]:4d}")

# ========================================
# æ–‡å­—æ•°åˆ†æ
# ========================================

def analyze_text_length(data_dict):
    """æ–‡å­—æ•°ã‚’åˆ†æ"""
    print("\n[2/5] æ–‡å­—æ•°åˆ†æ...")
    
    # çµ±è¨ˆæƒ…å ±ã‚’åé›†
    stats = {}
    all_lengths = {'train': [], 'val': [], 'test': []}
    label_lengths = {0: [], 1: []}
    
    for name, data in data_dict.items():
        lengths = [len(item['text']) for item in data]
        all_lengths[name] = lengths
        
        stats[name] = {
            'min': np.min(lengths),
            'max': np.max(lengths),
            'mean': np.mean(lengths),
            'median': np.median(lengths)
        }
        
        # ãƒ©ãƒ™ãƒ«åˆ¥ã®æ–‡å­—æ•°
        for item in data:
            label_lengths[item['label']].append(len(item['text']))
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å…¨ä½“ã®æ–‡å­—æ•°åˆ†å¸ƒï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ï¼‰
    ax1 = axes[0, 0]
    for name, lengths in all_lengths.items():
        ax1.hist(lengths, bins=20, alpha=0.6, label=name.capitalize(), edgecolor='black')
    ax1.set_xlabel('Text Length (characters)', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.set_title('Text Length Distribution by Dataset', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. ãƒ©ãƒ™ãƒ«åˆ¥ã®æ–‡å­—æ•°åˆ†å¸ƒ
    ax2 = axes[0, 1]
    ax2.hist(label_lengths[0], bins=20, alpha=0.6, label='Clear (Label 0)', 
            color='#4CAF50', edgecolor='black')
    ax2.hist(label_lengths[1], bins=20, alpha=0.6, label='Ambiguous (Label 1)', 
            color='#FF9800', edgecolor='black')
    ax2.set_xlabel('Text Length (characters)', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('Text Length Distribution by Label', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # 3. ç®±ã²ã’å›³ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ï¼‰
    ax3 = axes[1, 0]
    data_for_box = [all_lengths['train'], all_lengths['val'], all_lengths['test']]
    bp = ax3.boxplot(data_for_box, labels=['Train', 'Val', 'Test'],
                     patch_artist=True, showmeans=True)
    for patch in bp['boxes']:
        patch.set_facecolor('#2196F3')
        patch.set_alpha(0.6)
    ax3.set_ylabel('Text Length (characters)', fontsize=12)
    ax3.set_title('Text Length Box Plot by Dataset', fontsize=14, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. ç®±ã²ã’å›³ï¼ˆãƒ©ãƒ™ãƒ«åˆ¥ï¼‰
    ax4 = axes[1, 1]
    data_for_box2 = [label_lengths[0], label_lengths[1]]
    bp2 = ax4.boxplot(data_for_box2, labels=['Clear (Label 0)', 'Ambiguous (Label 1)'],
                      patch_artist=True, showmeans=True)
    bp2['boxes'][0].set_facecolor('#4CAF50')
    bp2['boxes'][1].set_facecolor('#FF9800')
    for patch in bp2['boxes']:
        patch.set_alpha(0.6)
    ax4.set_ylabel('Text Length (characters)', fontsize=12)
    ax4.set_title('Text Length Box Plot by Label', fontsize=14, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('figures/text_length_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/text_length_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  æ–‡å­—æ•°çµ±è¨ˆ:")
    print(f"  {'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ':12s} {'æœ€å°å€¤':>8s} {'æœ€å¤§å€¤':>8s} {'å¹³å‡å€¤':>8s} {'ä¸­å¤®å€¤':>8s}")
    print("  " + "-"*50)
    for name, stat in stats.items():
        print(f"  {name:12s} {stat['min']:8.0f} {stat['max']:8.0f} "
              f"{stat['mean']:8.1f} {stat['median']:8.1f}")
    
    print(f"\n  ãƒ©ãƒ™ãƒ«åˆ¥å¹³å‡æ–‡å­—æ•°:")
    print(f"    Label 0ï¼ˆæ˜ç¢ºï¼‰: {np.mean(label_lengths[0]):.1f}æ–‡å­—")
    print(f"    Label 1ï¼ˆæ›–æ˜§ï¼‰: {np.mean(label_lengths[1]):.1f}æ–‡å­—")
    
    return stats

# ========================================
# é »å‡ºèªåˆ†æ
# ========================================

def analyze_frequent_words(data_dict):
    """é »å‡ºèªã‚’åˆ†æ"""
    print("\n[3/5] é »å‡ºèªåˆ†æ...")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_data = []
    for data in data_dict.values():
        all_data.extend(data)
    
    # ãƒ©ãƒ™ãƒ«åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
    texts_by_label = {0: [], 1: []}
    for item in all_data:
        texts_by_label[item['label']].append(item['text'])
    
    # é »å‡ºèªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ã«2æ–‡å­—ä»¥ä¸Šã®éƒ¨åˆ†æ–‡å­—åˆ—ã‚’æŠ½å‡ºï¼‰
    def extract_words(texts, label_name):
        # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        keywords = [
            'ä»Šæ—¥', 'æ˜æ—¥', '17æ™‚', 'åˆå‰', 'åˆå¾Œ', 'ã¾ã§', 'PDF', 'Excel',
            'ä½œæˆ', 'é€ä¿¡', 'ãƒ¡ãƒ¼ãƒ«', 'Slack', 'ç¢ºèª', 'å ±å‘Š', 'æå‡º',
            'æ—©ã‚ã«', 'ãªã‚‹æ—©', 'ä¾‹ã®', 'ã„ã¤ã‚‚ã®', 'å¯¾å¿œ', 'å‡¦ç†',
            'ã–ã£ã¨', 'é©å½“ã«', 'ã‚ˆã‚ã—ã', 'å¾Œã§', 'ã‚ã‚Œ', 'ã“ã‚Œ'
        ]
        
        word_counts = Counter()
        for text in texts:
            for keyword in keywords:
                if keyword in text:
                    word_counts[keyword] += 1
        
        return word_counts.most_common(15)
    
    # ãƒ©ãƒ™ãƒ«åˆ¥ã®é »å‡ºèª
    frequent_words = {}
    for label in [0, 1]:
        label_name = 'æ˜ç¢º' if label == 0 else 'æ›–æ˜§'
        frequent_words[label] = extract_words(texts_by_label[label], label_name)
    
    # å¯è¦–åŒ–ï¼ˆè‹±èªç‰ˆ - æ—¥æœ¬èªã®å˜èªã¯è¡¨ç¤ºã—ãªã„ï¼‰
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    colors = ['#4CAF50', '#FF9800']
    labels_names = ['Clear (Label 0)', 'Ambiguous (Label 1)']
    
    for idx, label in enumerate([0, 1]):
        ax = axes[idx]
        words_data = frequent_words[label]
        
        if words_data:
            # å˜èªã®ä»£ã‚ã‚Šã«é †ä½ã‚’è¡¨ç¤º
            counts = [w[1] for w in words_data[:10]]
            ranks = [f'Rank {i+1}' for i in range(len(counts))]
            
            bars = ax.barh(range(len(counts)), counts, color=colors[idx], alpha=0.8)
            ax.set_yticks(range(len(counts)))
            ax.set_yticklabels(ranks, fontsize=11)
            ax.set_xlabel('Frequency', fontsize=12)
            ax.set_title(f'{labels_names[idx]} - Top 10 Words\n(See frequent_words.md for details)', 
                        fontsize=14, fontweight='bold')
            ax.invert_yaxis()
            ax.grid(axis='x', alpha=0.3)
            
            # å€¤ã‚’è¡¨ç¤º
            for bar, count in zip(bars, counts):
                width = bar.get_width()
                ax.text(width, bar.get_y() + bar.get_height()/2.,
                       f' {count}',
                       ha='left', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('figures/frequent_words.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/frequent_words.png")
    plt.close()
    
    # çµæœè¡¨ç¤º
    print("\n  é »å‡ºèª TOP10:")
    for label in [0, 1]:
        label_name = 'æ˜ç¢º (Label 0)' if label == 0 else 'æ›–æ˜§ (Label 1)'
        print(f"\n  {label_name}:")
        for i, (word, count) in enumerate(frequent_words[label][:10], 1):
            print(f"    {i:2d}. {word:10s} ({count}å›)")
    
    return frequent_words

# ========================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºåˆ†æ
# ========================================

def analyze_tokenization(data_dict):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’åˆ†æ"""
    print("\n[4/5] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºåˆ†æ...")
    print("  BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    token_lengths = {'train': [], 'val': [], 'test': []}
    stats = {}
    
    for name, data in data_dict.items():
        print(f"  {name} ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
        for item in data:
            tokens = tokenizer.encode(item['text'], add_special_tokens=True)
            token_lengths[name].append(len(tokens))
        
        lengths = token_lengths[name]
        stats[name] = {
            'min': np.min(lengths),
            'max': np.max(lengths),
            'mean': np.mean(lengths),
            'median': np.median(lengths),
            'percentile_95': np.percentile(lengths, 95),
            'percentile_99': np.percentile(lengths, 99)
        }
    
    # æ¨å¥¨max_length
    all_tokens = []
    for lengths in token_lengths.values():
        all_tokens.extend(lengths)
    recommended_max_length = int(np.percentile(all_tokens, 95))
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ
    ax1 = axes[0]
    for name, lengths in token_lengths.items():
        ax1.hist(lengths, bins=30, alpha=0.6, label=name.capitalize(), edgecolor='black')
    ax1.axvline(recommended_max_length, color='red', linestyle='--', linewidth=2,
               label=f'Recommended max_length ({recommended_max_length})')
    ax1.set_xlabel('Token Length', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.set_title('Token Length Distribution by Dataset', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. ç´¯ç©åˆ†å¸ƒ
    ax2 = axes[1]
    for name, lengths in token_lengths.items():
        sorted_lengths = np.sort(lengths)
        cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths) * 100
        ax2.plot(sorted_lengths, cumulative, label=name.capitalize(), linewidth=2)
    
    ax2.axvline(recommended_max_length, color='red', linestyle='--', linewidth=2,
               label=f'95th percentile ({recommended_max_length})')
    ax2.axhline(95, color='gray', linestyle=':', alpha=0.5)
    ax2.set_xlabel('Token Length', fontsize=12)
    ax2.set_ylabel('Cumulative Percentage (%)', fontsize=12)
    ax2.set_title('Cumulative Token Length Distribution', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('figures/token_length_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/token_length_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ:")
    print(f"  {'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ':12s} {'æœ€å°':>6s} {'æœ€å¤§':>6s} {'å¹³å‡':>6s} {'ä¸­å¤®':>6s} "
          f"{'95%':>6s} {'99%':>6s}")
    print("  " + "-"*60)
    for name, stat in stats.items():
        print(f"  {name:12s} {stat['min']:6.0f} {stat['max']:6.0f} "
              f"{stat['mean']:6.1f} {stat['median']:6.0f} "
              f"{stat['percentile_95']:6.0f} {stat['percentile_99']:6.0f}")
    
    print(f"\n  âœ¨ æ¨å¥¨max_length: {recommended_max_length}")
    print(f"     (95%ã®ãƒ‡ãƒ¼ã‚¿ãŒã“ã®é•·ã•ä»¥ä¸‹ã«åã¾ã‚Šã¾ã™)")
    
    return stats, recommended_max_length

# ========================================
# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
# ========================================

def display_samples(data_dict):
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"""
    print("\n[5/5] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º...")
    
    # Trainãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
    train_data = data_dict['train']
    label_0_samples = [item for item in train_data if item['label'] == 0]
    label_1_samples = [item for item in train_data if item['label'] == 1]
    
    print("\n  ã€Label 0 - æ˜ç¢ºãªæŒ‡ç¤ºã€‘ã‚µãƒ³ãƒ—ãƒ«5ä»¶:")
    for i, item in enumerate(np.random.choice(label_0_samples, 5, replace=False), 1):
        print(f"\n  {i}. {item['text']}")
        print(f"     ç†ç”±: {item['reason']}")
    
    print("\n  ã€Label 1 - æ›–æ˜§ãªæŒ‡ç¤ºã€‘ã‚µãƒ³ãƒ—ãƒ«5ä»¶:")
    for i, item in enumerate(np.random.choice(label_1_samples, 5, replace=False), 1):
        print(f"\n  {i}. {item['text']}")
        print(f"     ç†ç”±: {item['reason']}")

# ========================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ========================================

def generate_frequent_words_md(frequent_words):
    """é »å‡ºèªMarkdownãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ"""
    print("\n[é »å‡ºèªMarkdownãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ]")
    
    md_content = "# é »å‡ºèªåˆ†æ (Frequent Words Analysis)\n\n"
    md_content += "**æ³¨æ„**: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æ—¥æœ¬èªã®é »å‡ºèªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã‚°ãƒ©ãƒ•ã§ã¯æ–‡å­—åŒ–ã‘ã®ãŸã‚ã€ã“ã¡ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n"
    
    labels_info = {
        0: {'name': 'æ˜ç¢º (Clear)', 'emoji': 'âœ…', 'description': 'å…·ä½“çš„ã§æ˜ç¢ºãªæŒ‡ç¤ºæ–‡'},
        1: {'name': 'æ›–æ˜§ (Ambiguous)', 'emoji': 'âš ï¸', 'description': 'æŠ½è±¡çš„ã§æ›–æ˜§ãªæŒ‡ç¤ºæ–‡'}
    }
    
    for label in [0, 1]:
        info = labels_info[label]
        md_content += f"## {info['emoji']} {info['name']} - Label {label}\n\n"
        md_content += f"> {info['description']}\n\n"
        md_content += "| é †ä½ | å˜èª | å‡ºç¾å›æ•° | å‚™è€ƒ |\n"
        md_content += "|:----:|:-----|--------:|:-----|\n"
        
        words_data = frequent_words[label]
        for i, (word, count) in enumerate(words_data[:15], 1):
            # å‚™è€ƒè¿½åŠ ï¼ˆä¾‹ï¼‰
            note = ""
            if label == 0:
                if word in ['ã¾ã§', 'ã¾ã§ã«', 'æ—¥', 'æ™‚']:
                    note = "æœŸé™é–¢é€£"
                elif word in ['ä½œæˆ', 'æå‡º', 'ç¢ºèª', 'å ±å‘Š']:
                    note = "å…·ä½“çš„å‹•è©"
            else:
                if word in ['ã‚ˆã‚ã—ã', 'ãŠé¡˜ã„', 'ãªã‚‹æ—©', 'ã¡ã‚‡ã£ã¨']:
                    note = "æ›–æ˜§è¡¨ç¾"
                elif word in ['é©å®œ', 'ãªã‚“ã¨ã‹', 'ä¾‹ã®']:
                    note = "ä¸æ˜ç¢ºè¡¨ç¾"
            
            md_content += f"| {i} | {word} | {count:,} | {note} |\n"
        
        md_content += "\n"
    
    # æ¯”è¼ƒåˆ†æ
    md_content += "## ğŸ“Š æ¯”è¼ƒåˆ†æ\n\n"
    md_content += "### æ˜ç¢ºãªæŒ‡ç¤ºæ–‡ã®ç‰¹å¾´\n"
    md_content += "- æœŸé™ã‚’ç¤ºã™èªï¼ˆã€Œã¾ã§ã€ã€Œæ—¥ã€ã€Œæ™‚ã€ï¼‰ãŒå¤šãå‡ºç¾\n"
    md_content += "- å…·ä½“çš„ãªå‹•è©ï¼ˆã€Œä½œæˆã€ã€Œæå‡ºã€ã€Œç¢ºèªã€ï¼‰ãŒä½¿ç”¨ã•ã‚Œã‚‹\n"
    md_content += "- å›ºæœ‰åè©ã‚„å…·ä½“çš„ãªå¯¾è±¡ç‰©ãŒæ˜ç¤ºã•ã‚Œã‚‹\n\n"
    
    md_content += "### æ›–æ˜§ãªæŒ‡ç¤ºæ–‡ã®ç‰¹å¾´\n"
    md_content += "- æŠ½è±¡çš„ãªä¾é ¼è¡¨ç¾ï¼ˆã€Œã‚ˆã‚ã—ãã€ã€ŒãŠé¡˜ã„ã€ï¼‰ãŒé »å‡º\n"
    md_content += "- æ„Ÿè¦šçš„ãªå‰¯è©ï¼ˆã€Œã¡ã‚‡ã£ã¨ã€ã€Œãªã‚‹æ—©ã€ï¼‰ãŒå¤šç”¨ã•ã‚Œã‚‹\n"
    md_content += "- æŒ‡ç¤ºä»£åè©ï¼ˆã€Œã‚ã‚Œã€ã€Œä¾‹ã®ã€ï¼‰ãŒä½¿ç”¨ã•ã‚Œã‚‹\n\n"
    
    # ä¿å­˜
    with open('frequent_words.md', 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"  âœ“ ä¿å­˜: frequent_words.md")
    return md_content

def generate_html_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words):
    """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("\n[HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ]")
    
    html = """<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EDA Report - ãƒ“ã‚¸ãƒã‚¹æŒ‡ç¤ºæ–‡ æ›–æ˜§æ€§åˆ¤å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</title>
    <style>
        body {
            font-family: 'Segoe UI', 'Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'Yu Gothic', sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        .container {
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 4px solid #667eea;
            padding-bottom: 15px;
            font-size: 2.5em;
            margin-bottom: 30px;
        }
        h2 {
            color: #34495e;
            margin-top: 40px;
            border-left: 5px solid #667eea;
            padding-left: 15px;
        }
        h3 {
            color: #555;
            margin-top: 30px;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        .stat-card h3 {
            color: white;
            margin-top: 0;
            font-size: 1.1em;
        }
        .stat-card .value {
            font-size: 2.5em;
            font-weight: bold;
            margin: 10px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: bold;
        }
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
        .label-0 {
            background-color: #e8f5e9;
            border-left: 4px solid #4CAF50;
        }
        .label-1 {
            background-color: #fff3e0;
            border-left: 4px solid #FF9800;
        }
        .figure {
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 100%;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.15);
        }
        .recommendation {
            background: #fff9c4;
            border-left: 5px solid #fbc02d;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .badge {
            display: inline-block;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: bold;
            margin: 5px;
        }
        .badge-clear {
            background-color: #4CAF50;
            color: white;
        }
        .badge-ambiguous {
            background-color: #FF9800;
            color: white;
        }
        .emoji {
            font-size: 1.5em;
            margin-right: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (EDA Report)</h1>
        <p style="font-size: 1.1em; color: #666;">
            ãƒ“ã‚¸ãƒã‚¹æŒ‡ç¤ºæ–‡ æ›–æ˜§æ€§åˆ¤å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ - 2025å¹´12æœˆ12æ—¥ç”Ÿæˆ
        </p>
"""
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦
    total_samples = sum(len(data) for data in data_dict.values())
    html += """
        <h2>ğŸ—‚ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦</h2>
        <div class="stats-grid">
"""
    
    for name, data in data_dict.items():
        label_counts = Counter([item['label'] for item in data])
        html += f"""
            <div class="stat-card">
                <h3>{name.upper()} Dataset</h3>
                <div class="value">{len(data)}</div>
                <p>ã‚µãƒ³ãƒ—ãƒ«æ•°</p>
                <p style="font-size: 0.9em; margin-top: 10px;">
                    <span class="badge badge-clear">{label_counts[0]} Clear</span>
                    <span class="badge badge-ambiguous">{label_counts[1]} Ambiguous</span>
                </p>
            </div>
"""
    
    html += f"""
        </div>
        <p><strong>ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}</strong></p>
"""
    
    # æ–‡å­—æ•°çµ±è¨ˆ
    html += """
        <h2>ğŸ“ æ–‡å­—æ•°çµ±è¨ˆ</h2>
        <table>
            <tr>
                <th>çµ±è¨ˆé‡</th>
                <th>Train</th>
                <th>Val</th>
                <th>Test</th>
            </tr>
"""
    
    for stat in ['mean', 'std', 'min', 'max', 'median']:
        html += f"<tr><td><strong>{stat.upper()}</strong></td>"
        for dataset in ['train', 'val', 'test']:
            value = text_stats[dataset][stat]
            html += f"<td>{value:.1f}</td>"
        html += "</tr>\n"
    
    html += "</table>\n"
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ
    html += """
        <h2>ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ (BERT Tokenizer)</h2>
        <table>
            <tr>
                <th>çµ±è¨ˆé‡</th>
                <th>Train</th>
                <th>Val</th>
                <th>Test</th>
            </tr>
"""
    
    for stat in ['mean', 'std', 'min', 'max', 'p95']:
        html += f"<tr><td><strong>{stat.upper()}</strong></td>"
        for dataset in ['train', 'val', 'test']:
            value = token_stats[dataset][stat]
            html += f"<td>{value:.1f}</td>"
        html += "</tr>\n"
    
    html += "</table>\n"
    
    # æ¨å¥¨max_length
    html += f"""
        <div class="recommendation">
            <h3>ğŸ’¡ æ¨å¥¨è¨­å®š</h3>
            <p style="font-size: 1.2em;">
                <strong>max_length = {recommended_max_length}</strong>
            </p>
            <p>ã“ã®å€¤ã¯95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã«åŸºã¥ã„ã¦ãŠã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®95%ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚</p>
        </div>
"""
    
    # é »å‡ºèª
    html += """
        <h2>ğŸ” é »å‡ºèªåˆ†æ</h2>
        <h3><span class="emoji">âœ…</span>æ˜ç¢º (Clear) - Label 0</h3>
        <table class="label-0">
            <tr>
                <th>é †ä½</th>
                <th>å˜èª</th>
                <th>å‡ºç¾å›æ•°</th>
            </tr>
"""
    
    for i, (word, count) in enumerate(frequent_words[0][:15], 1):
        html += f"<tr><td>{i}</td><td><strong>{word}</strong></td><td>{count:,}</td></tr>\n"
    
    html += """
        </table>
        <h3><span class="emoji">âš ï¸</span>æ›–æ˜§ (Ambiguous) - Label 1</h3>
        <table class="label-1">
            <tr>
                <th>é †ä½</th>
                <th>å˜èª</th>
                <th>å‡ºç¾å›æ•°</th>
            </tr>
"""
    
    for i, (word, count) in enumerate(frequent_words[1][:15], 1):
        html += f"<tr><td>{i}</td><td><strong>{word}</strong></td><td>{count:,}</td></tr>\n"
    
    html += """
        </table>
"""
    
    # ã‚°ãƒ©ãƒ•
    html += """
        <h2>ğŸ“ˆ å¯è¦–åŒ–</h2>
        <div class="figure">
            <h3>ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ</h3>
            <img src="figures/label_distribution.png" alt="Label Distribution">
        </div>
        <div class="figure">
            <h3>æ–‡å­—æ•°åˆ†å¸ƒ</h3>
            <img src="figures/text_length_distribution.png" alt="Text Length Distribution">
        </div>
        <div class="figure">
            <h3>é »å‡ºèª TOP10</h3>
            <img src="figures/frequent_words.png" alt="Frequent Words">
            <p style="color: #666; font-size: 0.9em;">â€» ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªã¯ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç¢ºèªã—ã¦ãã ã•ã„</p>
        </div>
        <div class="figure">
            <h3>ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ</h3>
            <img src="figures/token_length_distribution.png" alt="Token Length Distribution">
        </div>
"""
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    html += """
        <h2>ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h2>
        <h3>âœ… æ˜ç¢ºãªæŒ‡ç¤ºæ–‡ã®ä¾‹</h3>
"""
    
    clear_samples = [item for item in data_dict['train'] if item['label'] == 0][:3]
    for i, sample in enumerate(clear_samples, 1):
        html += f"""
        <div class="label-0" style="padding: 15px; margin: 10px 0; border-radius: 5px;">
            <p><strong>ä¾‹ {i}:</strong> {sample['text']}</p>
            <p style="font-size: 0.9em; color: #666;"><em>ç†ç”±: {sample['reason']}</em></p>
        </div>
"""
    
    html += """
        <h3>âš ï¸ æ›–æ˜§ãªæŒ‡ç¤ºæ–‡ã®ä¾‹</h3>
"""
    
    ambiguous_samples = [item for item in data_dict['train'] if item['label'] == 1][:3]
    for i, sample in enumerate(ambiguous_samples, 1):
        html += f"""
        <div class="label-1" style="padding: 15px; margin: 10px 0; border-radius: 5px;">
            <p><strong>ä¾‹ {i}:</strong> {sample['text']}</p>
            <p style="font-size: 0.9em; color: #666;"><em>ç†ç”±: {sample['reason']}</em></p>
        </div>
"""
    
    html += """
        <hr style="margin: 40px 0;">
        <p style="text-align: center; color: #999;">
            Generated by 01_eda.py - ãƒ“ã‚¸ãƒã‚¹æŒ‡ç¤ºæ–‡ æ›–æ˜§æ€§åˆ¤å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        </p>
    </div>
</body>
</html>
"""
    
    with open('eda_report.html', 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"  âœ“ ä¿å­˜: eda_report.html")

def generate_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words):
    """EDAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("\n[ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ]")
    
    report = f"""# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚µãƒ³ãƒ—ãƒ«æ•° | Label 0 | Label 1 |
|------------|----------|---------|---------|
"""
    
    for name, data in data_dict.items():
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        report += f"| {name.capitalize()} | {len(data)} | {label_counts[0]} | {label_counts[1]} |\n"
    
    report += f"""
## æ–‡å­—æ•°çµ±è¨ˆ

| çµ±è¨ˆé‡ | Train | Val | Test |
|-------|-------|-----|------|
| æœ€å°å€¤ | {text_stats['train']['min']:.0f} | {text_stats['val']['min']:.0f} | {text_stats['test']['min']:.0f} |
| æœ€å¤§å€¤ | {text_stats['train']['max']:.0f} | {text_stats['val']['max']:.0f} | {text_stats['test']['max']:.0f} |
| å¹³å‡å€¤ | {text_stats['train']['mean']:.1f} | {text_stats['val']['mean']:.1f} | {text_stats['test']['mean']:.1f} |
| ä¸­å¤®å€¤ | {text_stats['train']['median']:.1f} | {text_stats['val']['median']:.1f} | {text_stats['test']['median']:.1f} |

## ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ

| çµ±è¨ˆé‡ | Train | Val | Test |
|-------|-------|-----|------|
| æœ€å°å€¤ | {token_stats['train']['min']:.0f} | {token_stats['val']['min']:.0f} | {token_stats['test']['min']:.0f} |
| æœ€å¤§å€¤ | {token_stats['train']['max']:.0f} | {token_stats['val']['max']:.0f} | {token_stats['test']['max']:.0f} |
| å¹³å‡å€¤ | {token_stats['train']['mean']:.1f} | {token_stats['val']['mean']:.1f} | {token_stats['test']['mean']:.1f} |
| 95%ã‚¿ã‚¤ãƒ« | {token_stats['train']['percentile_95']:.0f} | {token_stats['val']['percentile_95']:.0f} | {token_stats['test']['percentile_95']:.0f} |

**æ¨å¥¨max_length**: {recommended_max_length}

## é »å‡ºèªTOP10

### Label 0ï¼ˆæ˜ç¢ºãªæŒ‡ç¤ºï¼‰
"""
    
    for i, (word, count) in enumerate(frequent_words[0][:10], 1):
        report += f"{i}. {word} ({count}å›)\n"
    
    report += """
### Label 1ï¼ˆæ›–æ˜§ãªæŒ‡ç¤ºï¼‰
"""
    
    for i, (word, count) in enumerate(frequent_words[1][:10], 1):
        report += f"{i}. {word} ({count}å›)\n"
    
    report += f"""
## æ‰€è¦‹

- **ãƒ©ãƒ™ãƒ«ãƒãƒ©ãƒ³ã‚¹**: âœ… å®Œå…¨ã«50:50ã§å‡ç­‰
- **æ–‡å­—æ•°åˆ†å¸ƒ**: Train/Val/Testã§ä¸€è²«æ€§ã‚ã‚Šã€‚å¹³å‡{text_stats['train']['mean']:.1f}æ–‡å­—
- **ãƒˆãƒ¼ã‚¯ãƒ³é•·**: 95%ã®ãƒ‡ãƒ¼ã‚¿ãŒ{recommended_max_length}ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸‹ã«åã¾ã‚‹
- **é »å‡ºèªã®ç‰¹å¾´**:
  - Label 0ï¼ˆæ˜ç¢ºï¼‰: ã€Œä»Šæ—¥ã€ã€Œã¾ã§ã€ã€Œä½œæˆã€ã€Œé€ä¿¡ã€ãªã©å…·ä½“çš„ãªè¡Œå‹•ãƒ»æœŸé™ã‚’ç¤ºã™èªãŒå¤šã„
  - Label 1ï¼ˆæ›–æ˜§ï¼‰: ã€Œæ—©ã‚ã«ã€ã€Œä¾‹ã®ã€ã€Œå¯¾å¿œã€ã€Œå‡¦ç†ã€ãªã©æŠ½è±¡çš„ãƒ»æŒ‡ç¤ºä»£åè©ãŒå¤šã„

## æ¬¡ã®ã‚³ãƒã¸ã®æ¨å¥¨äº‹é …

- **max_lengthè¨­å®š**: {recommended_max_length} ã‚’æ¨å¥¨
  - 95%ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚«ãƒãƒ¼ã§ãã€ç„¡é§„ãª padding ã‚‚æœ€å°é™
- **batch_sizeæ¨å¥¨**: 16ï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
  - GPUä½¿ç”¨æ™‚: 16-32
  - CPUä½¿ç”¨æ™‚: 8-16
- **ç‰¹ã«æ³¨æ„ã™ã¹ããƒ‡ãƒ¼ã‚¿**: 
  - æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ10æ–‡å­—æœªæº€ï¼‰ã‚„é•·ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ50æ–‡å­—ä»¥ä¸Šï¼‰ãŒå°‘æ•°å­˜åœ¨
  - ãŸã ã—ã€å…¨ä½“çš„ã«ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹

## å¯è¦–åŒ–çµæœ

1. `figures/label_distribution.png` - ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
2. `figures/text_length_distribution.png` - æ–‡å­—æ•°åˆ†å¸ƒ
3. `figures/frequent_words.png` - é »å‡ºèªåˆ†æ
4. `figures/token_length_distribution.png` - ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ

---

**ä½œæˆæ—¥**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    with open('eda_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("  âœ“ ä¿å­˜: eda_report.md")

# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

if __name__ == '__main__':
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    train_data = load_jsonl('train.jsonl')
    val_data = load_jsonl('val.jsonl')
    test_data = load_jsonl('test.jsonl')
    
    data_dict = {
        'train': train_data,
        'val': val_data,
        'test': test_data
    }
    
    print(f"  âœ“ Train: {len(train_data)}ä»¶")
    print(f"  âœ“ Val: {len(val_data)}ä»¶")
    print(f"  âœ“ Test: {len(test_data)}ä»¶")
    
    # å„åˆ†æã‚’å®Ÿè¡Œ
    plot_label_distribution(data_dict)
    text_stats = analyze_text_length(data_dict)
    frequent_words = analyze_frequent_words(data_dict)
    token_stats, recommended_max_length = analyze_tokenization(data_dict)
    display_samples(data_dict)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words)
    
    # é »å‡ºèªMarkdownãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ
    generate_frequent_words_md(frequent_words)
    
    # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_html_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words)
    
    print("\n" + "="*60)
    print("âœ… EDAå®Œäº†ï¼")
    print("="*60)
    print("\næˆæœç‰©:")
    print("  - figures/label_distribution.png (è‹±èªç‰ˆ)")
    print("  - figures/text_length_distribution.png (è‹±èªç‰ˆ)")
    print("  - figures/frequent_words.png (è‹±èªç‰ˆ - Rankè¡¨ç¤º)")
    print("  - figures/token_length_distribution.png (è‹±èªç‰ˆ)")
    print("  - eda_report.md (è‹±èªç‰ˆãƒ¬ãƒãƒ¼ãƒˆ)")
    print("  - frequent_words.md (æ—¥æœ¬èªé »å‡ºèªãƒ†ãƒ¼ãƒ–ãƒ«)")
    print("  - eda_report.html (æ—¥æœ¬èªHTMLãƒ¬ãƒãƒ¼ãƒˆ)")
    print(f"\nğŸ“Œ æ¨å¥¨max_length: {recommended_max_length}")
    print("\nğŸ’¡ æ—¥æœ¬èªè¡¨ç¤º:")
    print("   - Markdown: frequent_words.md ã‚’å‚ç…§")
    print("   - HTML: eda_report.html ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚³ãƒ2ã§BERTå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")










