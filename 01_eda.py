"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from transformers import BertJapaneseTokenizer
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Noto Sans CJK JP'
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®æ–‡å­—åŒ–ã‘å¯¾ç­–

# Seabornã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# figuresãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
os.makedirs('figures', exist_ok=True)

print("="*60)
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰")
print("="*60)

# ========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# ========================================

def load_jsonl(filepath):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

# ========================================
# ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®å¯è¦–åŒ–
# ========================================

def plot_label_distribution(data_dict):
    """ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’å¯è¦–åŒ–"""
    print("\n[1/5] ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®åˆ†æ...")
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for idx, (name, data) in enumerate(data_dict.items()):
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        
        ax = axes[idx]
        colors = ['#4CAF50', '#FF9800']
        bars = ax.bar(['æ˜ç¢º (Label 0)', 'æ›–æ˜§ (Label 1)'], 
                      [label_counts[0], label_counts[1]],
                      color=colors, alpha=0.8)
        
        ax.set_ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°', fontsize=12)
        ax.set_title(f'{name.capitalize()} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n(ç·æ•°: {len(data)}ä»¶)', 
                    fontsize=14, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}',
                   ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('figures/label_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/label_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    for name, data in data_dict.items():
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        print(f"    {name:12s}: ç·æ•°={len(data):4d}, Label 0={label_counts[0]:4d}, Label 1={label_counts[1]:4d}")

# ========================================
# æ–‡å­—æ•°åˆ†æ
# ========================================

def analyze_text_length(data_dict):
    """æ–‡å­—æ•°ã‚’åˆ†æ"""
    print("\n[2/5] æ–‡å­—æ•°åˆ†æ...")
    
    # çµ±è¨ˆæƒ…å ±ã‚’åé›†
    stats = {}
    all_lengths = {'train': [], 'val': [], 'test': []}
    label_lengths = {0: [], 1: []}
    
    for name, data in data_dict.items():
        lengths = [len(item['text']) for item in data]
        all_lengths[name] = lengths
        
        stats[name] = {
            'min': np.min(lengths),
            'max': np.max(lengths),
            'mean': np.mean(lengths),
            'median': np.median(lengths)
        }
        
        # ãƒ©ãƒ™ãƒ«åˆ¥ã®æ–‡å­—æ•°
        for item in data:
            label_lengths[item['label']].append(len(item['text']))
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å…¨ä½“ã®æ–‡å­—æ•°åˆ†å¸ƒï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ï¼‰
    ax1 = axes[0, 0]
    for name, lengths in all_lengths.items():
        ax1.hist(lengths, bins=20, alpha=0.6, label=name.capitalize(), edgecolor='black')
    ax1.set_xlabel('æ–‡å­—æ•°', fontsize=12)
    ax1.set_ylabel('é »åº¦', fontsize=12)
    ax1.set_title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ æ–‡å­—æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. ãƒ©ãƒ™ãƒ«åˆ¥ã®æ–‡å­—æ•°åˆ†å¸ƒ
    ax2 = axes[0, 1]
    ax2.hist(label_lengths[0], bins=20, alpha=0.6, label='æ˜ç¢º (Label 0)', 
            color='#4CAF50', edgecolor='black')
    ax2.hist(label_lengths[1], bins=20, alpha=0.6, label='æ›–æ˜§ (Label 1)', 
            color='#FF9800', edgecolor='black')
    ax2.set_xlabel('æ–‡å­—æ•°', fontsize=12)
    ax2.set_ylabel('é »åº¦', fontsize=12)
    ax2.set_title('ãƒ©ãƒ™ãƒ«åˆ¥ æ–‡å­—æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # 3. ç®±ã²ã’å›³ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ï¼‰
    ax3 = axes[1, 0]
    data_for_box = [all_lengths['train'], all_lengths['val'], all_lengths['test']]
    bp = ax3.boxplot(data_for_box, labels=['Train', 'Val', 'Test'],
                     patch_artist=True, showmeans=True)
    for patch in bp['boxes']:
        patch.set_facecolor('#2196F3')
        patch.set_alpha(0.6)
    ax3.set_ylabel('æ–‡å­—æ•°', fontsize=12)
    ax3.set_title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ æ–‡å­—æ•°ã®ç®±ã²ã’å›³', fontsize=14, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. ç®±ã²ã’å›³ï¼ˆãƒ©ãƒ™ãƒ«åˆ¥ï¼‰
    ax4 = axes[1, 1]
    data_for_box2 = [label_lengths[0], label_lengths[1]]
    bp2 = ax4.boxplot(data_for_box2, labels=['æ˜ç¢º (Label 0)', 'æ›–æ˜§ (Label 1)'],
                      patch_artist=True, showmeans=True)
    bp2['boxes'][0].set_facecolor('#4CAF50')
    bp2['boxes'][1].set_facecolor('#FF9800')
    for patch in bp2['boxes']:
        patch.set_alpha(0.6)
    ax4.set_ylabel('æ–‡å­—æ•°', fontsize=12)
    ax4.set_title('ãƒ©ãƒ™ãƒ«åˆ¥ æ–‡å­—æ•°ã®ç®±ã²ã’å›³', fontsize=14, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('figures/text_length_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/text_length_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  æ–‡å­—æ•°çµ±è¨ˆ:")
    print(f"  {'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ':12s} {'æœ€å°å€¤':>8s} {'æœ€å¤§å€¤':>8s} {'å¹³å‡å€¤':>8s} {'ä¸­å¤®å€¤':>8s}")
    print("  " + "-"*50)
    for name, stat in stats.items():
        print(f"  {name:12s} {stat['min']:8.0f} {stat['max']:8.0f} "
              f"{stat['mean']:8.1f} {stat['median']:8.1f}")
    
    print(f"\n  ãƒ©ãƒ™ãƒ«åˆ¥å¹³å‡æ–‡å­—æ•°:")
    print(f"    Label 0ï¼ˆæ˜ç¢ºï¼‰: {np.mean(label_lengths[0]):.1f}æ–‡å­—")
    print(f"    Label 1ï¼ˆæ›–æ˜§ï¼‰: {np.mean(label_lengths[1]):.1f}æ–‡å­—")
    
    return stats

# ========================================
# é »å‡ºèªåˆ†æ
# ========================================

def analyze_frequent_words(data_dict):
    """é »å‡ºèªã‚’åˆ†æ"""
    print("\n[3/5] é »å‡ºèªåˆ†æ...")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_data = []
    for data in data_dict.values():
        all_data.extend(data)
    
    # ãƒ©ãƒ™ãƒ«åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
    texts_by_label = {0: [], 1: []}
    for item in all_data:
        texts_by_label[item['label']].append(item['text'])
    
    # é »å‡ºèªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ã«2æ–‡å­—ä»¥ä¸Šã®éƒ¨åˆ†æ–‡å­—åˆ—ã‚’æŠ½å‡ºï¼‰
    def extract_words(texts, label_name):
        # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        keywords = [
            'ä»Šæ—¥', 'æ˜æ—¥', '17æ™‚', 'åˆå‰', 'åˆå¾Œ', 'ã¾ã§', 'PDF', 'Excel',
            'ä½œæˆ', 'é€ä¿¡', 'ãƒ¡ãƒ¼ãƒ«', 'Slack', 'ç¢ºèª', 'å ±å‘Š', 'æå‡º',
            'æ—©ã‚ã«', 'ãªã‚‹æ—©', 'ä¾‹ã®', 'ã„ã¤ã‚‚ã®', 'å¯¾å¿œ', 'å‡¦ç†',
            'ã–ã£ã¨', 'é©å½“ã«', 'ã‚ˆã‚ã—ã', 'å¾Œã§', 'ã‚ã‚Œ', 'ã“ã‚Œ'
        ]
        
        word_counts = Counter()
        for text in texts:
            for keyword in keywords:
                if keyword in text:
                    word_counts[keyword] += 1
        
        return word_counts.most_common(15)
    
    # ãƒ©ãƒ™ãƒ«åˆ¥ã®é »å‡ºèª
    frequent_words = {}
    for label in [0, 1]:
        label_name = 'æ˜ç¢º' if label == 0 else 'æ›–æ˜§'
        frequent_words[label] = extract_words(texts_by_label[label], label_name)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    colors = ['#4CAF50', '#FF9800']
    labels_names = ['æ˜ç¢º (Label 0)', 'æ›–æ˜§ (Label 1)']
    
    for idx, label in enumerate([0, 1]):
        ax = axes[idx]
        words_data = frequent_words[label]
        
        if words_data:
            words = [w[0] for w in words_data[:10]]
            counts = [w[1] for w in words_data[:10]]
            
            bars = ax.barh(range(len(words)), counts, color=colors[idx], alpha=0.8)
            ax.set_yticks(range(len(words)))
            ax.set_yticklabels(words, fontsize=11)
            ax.set_xlabel('å‡ºç¾å›æ•°', fontsize=12)
            ax.set_title(f'{labels_names[idx]} é »å‡ºèª TOP10', 
                        fontsize=14, fontweight='bold')
            ax.invert_yaxis()
            ax.grid(axis='x', alpha=0.3)
            
            # å€¤ã‚’è¡¨ç¤º
            for bar, count in zip(bars, counts):
                width = bar.get_width()
                ax.text(width, bar.get_y() + bar.get_height()/2.,
                       f' {count}',
                       ha='left', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('figures/frequent_words.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/frequent_words.png")
    plt.close()
    
    # çµæœè¡¨ç¤º
    print("\n  é »å‡ºèª TOP10:")
    for label in [0, 1]:
        label_name = 'æ˜ç¢º (Label 0)' if label == 0 else 'æ›–æ˜§ (Label 1)'
        print(f"\n  {label_name}:")
        for i, (word, count) in enumerate(frequent_words[label][:10], 1):
            print(f"    {i:2d}. {word:10s} ({count}å›)")
    
    return frequent_words

# ========================================
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºåˆ†æ
# ========================================

def analyze_tokenization(data_dict):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’åˆ†æ"""
    print("\n[4/5] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºåˆ†æ...")
    print("  BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    token_lengths = {'train': [], 'val': [], 'test': []}
    stats = {}
    
    for name, data in data_dict.items():
        print(f"  {name} ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
        for item in data:
            tokens = tokenizer.encode(item['text'], add_special_tokens=True)
            token_lengths[name].append(len(tokens))
        
        lengths = token_lengths[name]
        stats[name] = {
            'min': np.min(lengths),
            'max': np.max(lengths),
            'mean': np.mean(lengths),
            'median': np.median(lengths),
            'percentile_95': np.percentile(lengths, 95),
            'percentile_99': np.percentile(lengths, 99)
        }
    
    # æ¨å¥¨max_length
    all_tokens = []
    for lengths in token_lengths.values():
        all_tokens.extend(lengths)
    recommended_max_length = int(np.percentile(all_tokens, 95))
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ
    ax1 = axes[0]
    for name, lengths in token_lengths.items():
        ax1.hist(lengths, bins=30, alpha=0.6, label=name.capitalize(), edgecolor='black')
    ax1.axvline(recommended_max_length, color='red', linestyle='--', linewidth=2,
               label=f'æ¨å¥¨max_length ({recommended_max_length})')
    ax1.set_xlabel('ãƒˆãƒ¼ã‚¯ãƒ³æ•°', fontsize=12)
    ax1.set_ylabel('é »åº¦', fontsize=12)
    ax1.set_title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. ç´¯ç©åˆ†å¸ƒ
    ax2 = axes[1]
    for name, lengths in token_lengths.items():
        sorted_lengths = np.sort(lengths)
        cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths) * 100
        ax2.plot(sorted_lengths, cumulative, label=name.capitalize(), linewidth=2)
    
    ax2.axvline(recommended_max_length, color='red', linestyle='--', linewidth=2,
               label=f'95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« ({recommended_max_length})')
    ax2.axhline(95, color='gray', linestyle=':', alpha=0.5)
    ax2.set_xlabel('ãƒˆãƒ¼ã‚¯ãƒ³æ•°', fontsize=12)
    ax2.set_ylabel('ç´¯ç©ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ (%)', fontsize=12)
    ax2.set_title('ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç´¯ç©åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('figures/token_length_distribution.png', dpi=150, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: figures/token_length_distribution.png")
    plt.close()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n  ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ:")
    print(f"  {'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ':12s} {'æœ€å°':>6s} {'æœ€å¤§':>6s} {'å¹³å‡':>6s} {'ä¸­å¤®':>6s} "
          f"{'95%':>6s} {'99%':>6s}")
    print("  " + "-"*60)
    for name, stat in stats.items():
        print(f"  {name:12s} {stat['min']:6.0f} {stat['max']:6.0f} "
              f"{stat['mean']:6.1f} {stat['median']:6.0f} "
              f"{stat['percentile_95']:6.0f} {stat['percentile_99']:6.0f}")
    
    print(f"\n  âœ¨ æ¨å¥¨max_length: {recommended_max_length}")
    print(f"     (95%ã®ãƒ‡ãƒ¼ã‚¿ãŒã“ã®é•·ã•ä»¥ä¸‹ã«åã¾ã‚Šã¾ã™)")
    
    return stats, recommended_max_length

# ========================================
# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
# ========================================

def display_samples(data_dict):
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"""
    print("\n[5/5] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º...")
    
    # Trainãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
    train_data = data_dict['train']
    label_0_samples = [item for item in train_data if item['label'] == 0]
    label_1_samples = [item for item in train_data if item['label'] == 1]
    
    print("\n  ã€Label 0 - æ˜ç¢ºãªæŒ‡ç¤ºã€‘ã‚µãƒ³ãƒ—ãƒ«5ä»¶:")
    for i, item in enumerate(np.random.choice(label_0_samples, 5, replace=False), 1):
        print(f"\n  {i}. {item['text']}")
        print(f"     ç†ç”±: {item['reason']}")
    
    print("\n  ã€Label 1 - æ›–æ˜§ãªæŒ‡ç¤ºã€‘ã‚µãƒ³ãƒ—ãƒ«5ä»¶:")
    for i, item in enumerate(np.random.choice(label_1_samples, 5, replace=False), 1):
        print(f"\n  {i}. {item['text']}")
        print(f"     ç†ç”±: {item['reason']}")

# ========================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ========================================

def generate_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words):
    """EDAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("\n[ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ]")
    
    report = f"""# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚µãƒ³ãƒ—ãƒ«æ•° | Label 0 | Label 1 |
|------------|----------|---------|---------|
"""
    
    for name, data in data_dict.items():
        labels = [item['label'] for item in data]
        label_counts = Counter(labels)
        report += f"| {name.capitalize()} | {len(data)} | {label_counts[0]} | {label_counts[1]} |\n"
    
    report += f"""
## æ–‡å­—æ•°çµ±è¨ˆ

| çµ±è¨ˆé‡ | Train | Val | Test |
|-------|-------|-----|------|
| æœ€å°å€¤ | {text_stats['train']['min']:.0f} | {text_stats['val']['min']:.0f} | {text_stats['test']['min']:.0f} |
| æœ€å¤§å€¤ | {text_stats['train']['max']:.0f} | {text_stats['val']['max']:.0f} | {text_stats['test']['max']:.0f} |
| å¹³å‡å€¤ | {text_stats['train']['mean']:.1f} | {text_stats['val']['mean']:.1f} | {text_stats['test']['mean']:.1f} |
| ä¸­å¤®å€¤ | {text_stats['train']['median']:.1f} | {text_stats['val']['median']:.1f} | {text_stats['test']['median']:.1f} |

## ãƒˆãƒ¼ã‚¯ãƒ³æ•°çµ±è¨ˆ

| çµ±è¨ˆé‡ | Train | Val | Test |
|-------|-------|-----|------|
| æœ€å°å€¤ | {token_stats['train']['min']:.0f} | {token_stats['val']['min']:.0f} | {token_stats['test']['min']:.0f} |
| æœ€å¤§å€¤ | {token_stats['train']['max']:.0f} | {token_stats['val']['max']:.0f} | {token_stats['test']['max']:.0f} |
| å¹³å‡å€¤ | {token_stats['train']['mean']:.1f} | {token_stats['val']['mean']:.1f} | {token_stats['test']['mean']:.1f} |
| 95%ã‚¿ã‚¤ãƒ« | {token_stats['train']['percentile_95']:.0f} | {token_stats['val']['percentile_95']:.0f} | {token_stats['test']['percentile_95']:.0f} |

**æ¨å¥¨max_length**: {recommended_max_length}

## é »å‡ºèªTOP10

### Label 0ï¼ˆæ˜ç¢ºãªæŒ‡ç¤ºï¼‰
"""
    
    for i, (word, count) in enumerate(frequent_words[0][:10], 1):
        report += f"{i}. {word} ({count}å›)\n"
    
    report += """
### Label 1ï¼ˆæ›–æ˜§ãªæŒ‡ç¤ºï¼‰
"""
    
    for i, (word, count) in enumerate(frequent_words[1][:10], 1):
        report += f"{i}. {word} ({count}å›)\n"
    
    report += f"""
## æ‰€è¦‹

- **ãƒ©ãƒ™ãƒ«ãƒãƒ©ãƒ³ã‚¹**: âœ… å®Œå…¨ã«50:50ã§å‡ç­‰
- **æ–‡å­—æ•°åˆ†å¸ƒ**: Train/Val/Testã§ä¸€è²«æ€§ã‚ã‚Šã€‚å¹³å‡{text_stats['train']['mean']:.1f}æ–‡å­—
- **ãƒˆãƒ¼ã‚¯ãƒ³é•·**: 95%ã®ãƒ‡ãƒ¼ã‚¿ãŒ{recommended_max_length}ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸‹ã«åã¾ã‚‹
- **é »å‡ºèªã®ç‰¹å¾´**:
  - Label 0ï¼ˆæ˜ç¢ºï¼‰: ã€Œä»Šæ—¥ã€ã€Œã¾ã§ã€ã€Œä½œæˆã€ã€Œé€ä¿¡ã€ãªã©å…·ä½“çš„ãªè¡Œå‹•ãƒ»æœŸé™ã‚’ç¤ºã™èªãŒå¤šã„
  - Label 1ï¼ˆæ›–æ˜§ï¼‰: ã€Œæ—©ã‚ã«ã€ã€Œä¾‹ã®ã€ã€Œå¯¾å¿œã€ã€Œå‡¦ç†ã€ãªã©æŠ½è±¡çš„ãƒ»æŒ‡ç¤ºä»£åè©ãŒå¤šã„

## æ¬¡ã®ã‚³ãƒã¸ã®æ¨å¥¨äº‹é …

- **max_lengthè¨­å®š**: {recommended_max_length} ã‚’æ¨å¥¨
  - 95%ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚«ãƒãƒ¼ã§ãã€ç„¡é§„ãª padding ã‚‚æœ€å°é™
- **batch_sizeæ¨å¥¨**: 16ï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
  - GPUä½¿ç”¨æ™‚: 16-32
  - CPUä½¿ç”¨æ™‚: 8-16
- **ç‰¹ã«æ³¨æ„ã™ã¹ããƒ‡ãƒ¼ã‚¿**: 
  - æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ10æ–‡å­—æœªæº€ï¼‰ã‚„é•·ã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ50æ–‡å­—ä»¥ä¸Šï¼‰ãŒå°‘æ•°å­˜åœ¨
  - ãŸã ã—ã€å…¨ä½“çš„ã«ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹

## å¯è¦–åŒ–çµæœ

1. `figures/label_distribution.png` - ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
2. `figures/text_length_distribution.png` - æ–‡å­—æ•°åˆ†å¸ƒ
3. `figures/frequent_words.png` - é »å‡ºèªåˆ†æ
4. `figures/token_length_distribution.png` - ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†å¸ƒ

---

**ä½œæˆæ—¥**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    with open('eda_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("  âœ“ ä¿å­˜: eda_report.md")

# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

if __name__ == '__main__':
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    train_data = load_jsonl('train.jsonl')
    val_data = load_jsonl('val.jsonl')
    test_data = load_jsonl('test.jsonl')
    
    data_dict = {
        'train': train_data,
        'val': val_data,
        'test': test_data
    }
    
    print(f"  âœ“ Train: {len(train_data)}ä»¶")
    print(f"  âœ“ Val: {len(val_data)}ä»¶")
    print(f"  âœ“ Test: {len(test_data)}ä»¶")
    
    # å„åˆ†æã‚’å®Ÿè¡Œ
    plot_label_distribution(data_dict)
    text_stats = analyze_text_length(data_dict)
    frequent_words = analyze_frequent_words(data_dict)
    token_stats, recommended_max_length = analyze_tokenization(data_dict)
    display_samples(data_dict)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(data_dict, text_stats, token_stats, recommended_max_length, frequent_words)
    
    print("\n" + "="*60)
    print("âœ… EDAå®Œäº†ï¼")
    print("="*60)
    print("\næˆæœç‰©:")
    print("  - figures/label_distribution.png")
    print("  - figures/text_length_distribution.png")
    print("  - figures/frequent_words.png")
    print("  - figures/token_length_distribution.png")
    print("  - eda_report.md")
    print(f"\nğŸ“Œ æ¨å¥¨max_length: {recommended_max_length}")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚³ãƒ2ã§BERTå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
