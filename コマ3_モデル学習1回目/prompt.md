# ã‚³ãƒ3å®Ÿè£…ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆ1å›ç›®ï¼‰

ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦ã€åˆå›å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

---

## ğŸš€ å­¦ç¿’å®Ÿè¡Œæ‰‹é †

### ã‚¹ãƒ†ãƒƒãƒ—1: äº‹å‰ç¢ºèª

ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ã€ç’°å¢ƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

```python
# pre_check.py
import os
import torch
import json

print("="*60)
print("å­¦ç¿’å‰ãƒã‚§ãƒƒã‚¯")
print("="*60)

# 1. ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
import shutil
total, used, free = shutil.disk_usage("/")
print(f"\nãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡:")
print(f"  ç©ºãå®¹é‡: {free // (2**30)} GB")

# 2. GPU/CPUãƒ¡ãƒ¢ãƒª
if torch.cuda.is_available():
    print(f"\nGPUæƒ…å ±:")
    print(f"  GPUå: {torch.cuda.get_device_name(0)}")
    print(f"  ç·ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("\nCPUãƒ¢ãƒ¼ãƒ‰ã§å­¦ç¿’ã—ã¾ã™")

# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª
datasets = ['train.jsonl', 'val.jsonl', 'test.jsonl']
for ds in datasets:
    if os.path.exists(ds):
        with open(ds, 'r') as f:
            count = sum(1 for _ in f)
        print(f"  {ds}: {count}ä»¶ âœ…")
    else:
        print(f"  {ds}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ âŒ")

# 4. ã‚¹ã‚¯ãƒªãƒ—ãƒˆç¢ºèª
if os.path.exists('02_train_bert.py'):
    print("\n02_train_bert.py: å­˜åœ¨ã—ã¾ã™ âœ…")
else:
    print("\n02_train_bert.py: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ âŒ")

print("\næº–å‚™å®Œäº†ï¼å­¦ç¿’ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
```

å®Ÿè¡Œï¼š
```bash
python pre_check.py
```

---

### ã‚¹ãƒ†ãƒƒãƒ—2: å­¦ç¿’å®Ÿè¡Œ

```bash
python 02_train_bert.py
```

**å­¦ç¿’ä¸­ã®ç¢ºèªãƒã‚¤ãƒ³ãƒˆ**ï¼š

1. **åˆæœŸè¨­å®šãŒè¡¨ç¤ºã•ã‚Œã‚‹**
   - ãƒ¢ãƒ‡ãƒ«å
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º
   - ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU/CPUï¼‰

2. **å­¦ç¿’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼**
   - Epoch 1/3, 2/3, 3/3 ã¨é€²ã‚€
   - LossãŒå¾ã€…ã«ä¸‹ãŒã‚‹ã“ã¨ã‚’ç¢ºèª

3. **Validationè©•ä¾¡**
   - å„Epochå¾Œã«ValidationãŒå®Ÿè¡Œã•ã‚Œã‚‹
   - AccuracyãŒä¸ŠãŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

4. **ã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹ç›£è¦–**
   - CUDA out of memory â†’ batch_sizeã‚’æ¸›ã‚‰ã—ã¦å†å®Ÿè¡Œ
   - ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ â†’ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²

---

### ã‚¹ãƒ†ãƒƒãƒ—3: èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®æŠ½å‡º

å­¦ç¿’å®Œäº†å¾Œã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

```python
# extract_errors.py
"""
èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import numpy as np
import torch
from transformers import BertJapaneseTokenizer, BertForSequenceClassification

def load_jsonl(filepath):
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def predict(texts, model, tokenizer, device):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’äºˆæ¸¬
    """
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors='pt'
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=1)
    
    return predictions.cpu().numpy()

def main():
    print("èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    model = BertForSequenceClassification.from_pretrained('./saved_model')
    tokenizer = BertJapaneseTokenizer.from_pretrained('./saved_model')
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    # Testãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_data = load_jsonl('test.jsonl')
    texts = [item['text'] for item in test_data]
    true_labels = [item['label'] for item in test_data]
    
    # äºˆæ¸¬
    pred_labels = predict(texts, model, tokenizer, device)
    
    # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
    error_samples = []
    for i, (text, true_label, pred_label) in enumerate(zip(texts, true_labels, pred_labels)):
        if true_label != pred_label:
            error_samples.append({
                'id': i,
                'text': text,
                'true_label': int(true_label),
                'pred_label': int(pred_label),
                'true_label_name': 'æ˜ç¢º' if true_label == 0 else 'æ›–æ˜§',
                'pred_label_name': 'æ˜ç¢º' if pred_label == 0 else 'æ›–æ˜§'
            })
    
    # ä¿å­˜
    with open('error_samples.json', 'w', encoding='utf-8') as f:
        json.dump(error_samples, f, indent=2, ensure_ascii=False)
    
    print(f"\nèª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(error_samples)}/{len(test_data)}")
    print(f"æ­£è§£ç‡: {(len(test_data) - len(error_samples)) / len(test_data) * 100:.2f}%")
    print(f"\nèª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’ error_samples.json ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    # ã„ãã¤ã‹è¡¨ç¤º
    print("\n--- èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ä¾‹ ---")
    for i, sample in enumerate(error_samples[:5]):
        print(f"\n{i+1}. {sample['text']}")
        print(f"   æ­£è§£: {sample['true_label_name']} â†’ äºˆæ¸¬: {sample['pred_label_name']}")

if __name__ == '__main__':
    main()
```

å®Ÿè¡Œï¼š
```bash
python extract_errors.py
```

---

### ã‚¹ãƒ†ãƒƒãƒ—4: åˆå›å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ

`first_training_report.md` ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

```markdown
# åˆå›å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ

## å­¦ç¿’è¨­å®š

- **ãƒ¢ãƒ‡ãƒ«**: cl-tohoku/bert-base-japanese-v3
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: train 1,600ä»¶ / val 200ä»¶ / test 200ä»¶
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
  - max_length: ___
  - batch_size: ___
  - learning_rate: ___
  - num_epochs: ___

## å­¦ç¿’æ™‚é–“

- é–‹å§‹æ™‚åˆ»: ___
- çµ‚äº†æ™‚åˆ»: ___
- æ‰€è¦æ™‚é–“: ___ åˆ†

## çµæœ

### Testè©•ä¾¡æŒ‡æ¨™

| æŒ‡æ¨™ | ã‚¹ã‚³ã‚¢ |
|------|--------|
| Accuracy | ___ % |
| F1ã‚¹ã‚³ã‚¢ | ___ |
| Precision | ___ |
| Recall | ___ |

### æ··åŒè¡Œåˆ—

ï¼ˆfigures/confusion_matrix.png ã‚’å‚ç…§ï¼‰

### èª¤åˆ†é¡åˆ†æ

- èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«æ•°: ___ / 200
- èª¤åˆ†é¡ç‡: ___ %

#### èª¤åˆ†é¡ã®å‚¾å‘

1. **Label 0â†’1 ã®èª¤åˆ†é¡ï¼ˆæ˜ç¢ºã‚’æ›–æ˜§ã¨åˆ¤å®šï¼‰**
   - ã‚µãƒ³ãƒ—ãƒ«æ•°: ___
   - ç‰¹å¾´: ___

2. **Label 1â†’0 ã®èª¤åˆ†é¡ï¼ˆæ›–æ˜§ã‚’æ˜ç¢ºã¨åˆ¤å®šï¼‰**
   - ã‚µãƒ³ãƒ—ãƒ«æ•°: ___
   - ç‰¹å¾´: ___

## è€ƒå¯Ÿ

### ã†ã¾ãã„ã£ãŸç‚¹

- ___

### æ”¹å–„ãŒå¿…è¦ãªç‚¹

- ___

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã‚³ãƒ4ã§å®Ÿæ–½ï¼‰

### æ”¹å–„æ¡ˆ

1. ___
2. ___
3. ___

### è¿½åŠ åˆ†æ

- ___
```

---

## ğŸ“Š çµæœã®èª­ã¿æ–¹

### ç›®æ¨™ç²¾åº¦ã®åˆ¤å®š

- **Accuracy 80%ä»¥ä¸Š**: ğŸ‰ ç´ æ™´ã‚‰ã—ã„ï¼
- **Accuracy 75-80%**: âœ… è‰¯å¥½ï¼
- **Accuracy 70-75%**: ğŸ˜ æ”¹å–„ã®ä½™åœ°ã‚ã‚Š
- **Accuracy 70%æœªæº€**: ğŸ˜¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦

### æ··åŒè¡Œåˆ—ã®èª­ã¿æ–¹

```
           äºˆæ¸¬:æ˜ç¢º  äºˆæ¸¬:æ›–æ˜§
æ­£è§£:æ˜ç¢º     90        10      â† æ˜ç¢ºãªã®ã«æ›–æ˜§ã¨èª¤åˆ¤å®šï¼ˆ10ä»¶ï¼‰
æ­£è§£:æ›–æ˜§     15        85      â† æ›–æ˜§ãªã®ã«æ˜ç¢ºã¨èª¤åˆ¤å®šï¼ˆ15ä»¶ï¼‰
```

- **å·¦ä¸Šï¼ˆTrue Positiveï¼‰**: æ˜ç¢ºã‚’æ˜ç¢ºã¨æ­£ã—ãåˆ¤å®š
- **å³ä¸‹ï¼ˆTrue Negativeï¼‰**: æ›–æ˜§ã‚’æ›–æ˜§ã¨æ­£ã—ãåˆ¤å®š
- **å³ä¸Šï¼ˆFalse Positiveï¼‰**: æ˜ç¢ºãªã®ã«æ›–æ˜§ã¨èª¤åˆ¤å®š
- **å·¦ä¸‹ï¼ˆFalse Negativeï¼‰**: æ›–æ˜§ãªã®ã«æ˜ç¢ºã¨èª¤åˆ¤å®š

---

## âœ… å®Œäº†ç¢ºèª

- [ ] å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ãŸ
- [ ] `training_results.json` ãŒç”Ÿæˆã•ã‚ŒãŸ
- [ ] `saved_model/` ãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚ŒãŸ
- [ ] æ··åŒè¡Œåˆ—ãŒä¿å­˜ã•ã‚ŒãŸ
- [ ] `error_samples.json` ãŒç”Ÿæˆã•ã‚ŒãŸ
- [ ] `first_training_report.md` ã‚’ä½œæˆã—ãŸ
- [ ] Test Accuracy ___ % ã‚’é”æˆ

---

## â­ï¸ æ¬¡ã®ã‚³ãƒï¼ˆã‚³ãƒ4ï¼‰ã¸ã®æº–å‚™

ä»¥ä¸‹ã‚’ãƒ¡ãƒ¢ã—ã¦ãã ã•ã„ï¼š

1. **é”æˆã—ãŸç²¾åº¦**: Accuracy ___ %
2. **èª¤åˆ†é¡ã®ä¸»ãªå‚¾å‘**: ___
3. **æ”¹å–„ã™ã¹ããƒã‚¤ãƒ³ãƒˆ**: ___

ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’æŒã£ã¦ã€ã‚³ãƒ4ã§ã‚¨ãƒ©ãƒ¼åˆ†æã¨æ”¹å–„ã‚’è¡Œã„ã¾ã™ã€‚










